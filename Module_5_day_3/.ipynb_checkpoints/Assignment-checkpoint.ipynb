{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7343c19a-1474-4a49-b32d-b8868e30386c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import pyspark\n",
    "from IPython.display import clear_output, display\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    avg,\n",
    "    col,\n",
    "    max,\n",
    "    min,\n",
    "    udf,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a90b92c0-ef1b-40e8-8b00-9ee033067515",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"TitanicData\").getOrCreate()\n",
    "# Read the CSV file into a DataFrame\n",
    "df_pyspark = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"header\", \"false\").csv(\"data/titanic.csv\")\n",
    "# Define the column names of the Titanic dataset\n",
    "column_names = ['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked','Timestamp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "152e8586-2fec-4c95-8f39-fda1ef175ba0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      " |-- Timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assign the column names to the DataFrame\n",
    "df_pyspark = df_pyspark.toDF(*column_names)\n",
    "# Display the DataFrame schema\n",
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08d9709b-396a-4ef0-9924-1606760909fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PassengerId',\n",
       " 'Survived',\n",
       " 'Pclass',\n",
       " 'Name',\n",
       " 'Sex',\n",
       " 'Age',\n",
       " 'SibSp',\n",
       " 'Parch',\n",
       " 'Ticket',\n",
       " 'Fare',\n",
       " 'Cabin',\n",
       " 'Embarked',\n",
       " 'Timestamp']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the column names\n",
    "df_pyspark.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6b02c38-558d-4baa-86ef-94ac556031c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                Name|\n",
      "+--------------------+\n",
      "|Braund, Mr. Owen ...|\n",
      "|Cumings, Mrs. Joh...|\n",
      "|Heikkinen, Miss. ...|\n",
      "|Futrelle, Mrs. Ja...|\n",
      "|Allen, Mr. Willia...|\n",
      "|    Moran, Mr. James|\n",
      "|McCarthy, Mr. Tim...|\n",
      "|Palsson, Master. ...|\n",
      "|Johnson, Mrs. Osc...|\n",
      "|Nasser, Mrs. Nich...|\n",
      "|Sandstrom, Miss. ...|\n",
      "|Bonnell, Miss. El...|\n",
      "|Saundercock, Mr. ...|\n",
      "|Andersson, Mr. An...|\n",
      "|Vestrom, Miss. Hu...|\n",
      "|Hewlett, Mrs. (Ma...|\n",
      "|Rice, Master. Eugene|\n",
      "|Williams, Mr. Cha...|\n",
      "|Vander Planke, Mr...|\n",
      "|Masselmani, Mrs. ...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.select(\"Name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d84ac32e-5ed4-4a1d-9544-59c7bac93477",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+------------------+------------------+------------------+-------------------+-----------------+\n",
      "|      PassengerId|           Survived|            Pclass|               Age|             SibSp|              Parch|             Fare|\n",
      "+-----------------+-------------------+------------------+------------------+------------------+-------------------+-----------------+\n",
      "|              891|                891|               891|               714|               891|                891|              891|\n",
      "|            446.0| 0.3838383838383838| 2.308641975308642|29.679271708683473|0.5230078563411896|0.38159371492704824| 32.2042079685746|\n",
      "|257.3538420152301|0.48659245426485753|0.8360712409770491|14.536482769437564|1.1027434322934315| 0.8060572211299488|49.69342859718089|\n",
      "|                1|                  0|                 1|                 0|                 0|                  0|              0.0|\n",
      "|              891|                  1|                 3|                80|                 8|                  6|         512.3292|\n",
      "+-----------------+-------------------+------------------+------------------+------------------+-------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_cols = [col for col, dtype in df_pyspark.dtypes if dtype in ['int', 'double']]\n",
    "num_stats = df_pyspark.select(num_cols).describe().drop(\"summary\")\n",
    "num_stats.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ff233ab-e5fc-4467-b0bf-a8d9bc09e88a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum values:\n",
      "Row(PassengerId=1, Survived=0, Pclass=1, Age=0, Fare=0.0, SibSp=0, Parch=0, Embarked='C')\n",
      "Maximum values:\n",
      "Row(PassengerId=891, Survived=1, Pclass=3, Age=80, Fare=512.3292, SibSp=8, Parch=6, Embarked='S')\n",
      "Average values:\n",
      "Row(PassengerId=446.0, Survived=0.3838383838383838, Pclass=2.308641975308642, Age=29.679271708683473, Fare=32.2042079685746, SibSp=0.5230078563411896, Parch=0.38159371492704824, Embarked=None)\n"
     ]
    }
   ],
   "source": [
    "numeric_columns = df_pyspark.select(\"PassengerId\", \"Survived\", \"Pclass\", \"Age\", \"Fare\", \"SibSp\", \"Parch\", \"Embarked\").columns\n",
    "min_values = df_pyspark.select(numeric_columns).agg(*(min(c).alias(c) for c in numeric_columns)).collect()[0]\n",
    "max_values = df_pyspark.select(numeric_columns).agg(*(max(c).alias(c) for c in numeric_columns)).collect()[0]\n",
    "avg_values = df_pyspark.select(numeric_columns).agg(*(avg(c).alias(c) for c in numeric_columns)).collect()[0]\n",
    "print(\"Minimum values:\")\n",
    "print(min_values)\n",
    "print(\"Maximum values:\")\n",
    "print(max_values)\n",
    "print(\"Average values:\")\n",
    "print(avg_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35a1bdf8-fb2e-4c1b-af9c-c1cdf5cae824",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def change_last_letter(x: str) -> StringType:\n",
    "  words = x.split()\n",
    "  for i in range(len(words)):\n",
    "    words[i] = words[i][:-1] + \"1\"\n",
    "  return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "519a50c0-3396-4874-b1e3-b83bb5037d11",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(PassengerId=1, Survived=0, Pclass=3, Name='Braund1 Mr1 Owe1 Harri1', Sex='male', Age=22, SibSp=1, Parch=0, Ticket='A/5 21171', Fare=7.25, Cabin=None, Embarked='S', Timestamp=datetime.datetime(2020, 1, 1, 13, 45, 25))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a UDF from the function\n",
    "udf_change_last_letter = udf(change_last_letter, StringType())\n",
    "# Apply the UDF to the \"Name\" column\n",
    "df = df_pyspark.withColumn(\"Name\", udf_change_last_letter(col(\"Name\")))\n",
    "# Print the results\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d5e1d48-f1ec-4bb6-a532-6f643f7b30b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sort DataFrame by the first column\n",
    "sorted_df = df.orderBy(df.columns[0])\n",
    "# Save the sorted DataFrame to Parquet file\n",
    "sorted_df.write.parquet(\"./sorted_titanic/titanic.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1279eda2-eb9e-45b4-a462-b20312d6dbe4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
